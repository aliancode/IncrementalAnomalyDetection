{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c3158-3aed-4d98-9e64-3c2a60a6dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 23:26:14 | INFO     | ðŸš€ Starting Advanced Experiment Framework\n",
      "2025-10-02 23:26:14 | INFO     | âœ… Raw NSL-KDD files found in: research_data\n",
      "2025-10-02 23:26:14 | INFO     | Loading NSL-KDD from .txt files...\n",
      "2025-10-02 23:26:18 | INFO     | Combined dataset: 148517 samples\n",
      "2025-10-02 23:26:18 | INFO     | After filtering: 148517 samples\n",
      "2025-10-02 23:26:24 | INFO     | Task 0: 10000 samples (9963 anomalies)\n",
      "2025-10-02 23:26:24 | INFO     | Task 1: 10000 samples (9830 anomalies)\n",
      "2025-10-02 23:26:25 | INFO     | Task 2: 10000 samples (9985 anomalies)\n",
      "2025-10-02 23:26:25 | INFO     | Starting hyperparameter tuning...\n",
      "2025-10-02 23:28:04 | INFO     | Best params for TADR-VAE: {'lr': 0.0008983801068631165, 'latent_dim': 16, 'hidden_dim': 256, 'kl_weight': 0.2933216713441632, 'task_embedding_dim': 8}\n",
      "2025-10-02 23:28:46 | INFO     | Best params for Vanilla VAE: {'lr': 0.008889988444746321, 'latent_dim': 32, 'hidden_dim': 128, 'kl_weight': 0.07761425986216605}\n",
      "2025-10-02 23:29:42 | INFO     | Best params for VAE+EWC: {'lr': 0.0010981659484934022, 'latent_dim': 32, 'hidden_dim': 256, 'kl_weight': 0.39134067207580153, 'ewc_lambda': 395.64595275464734}\n",
      "2025-10-02 23:29:42 | INFO     | Running experiments with seed: 42\n",
      "2025-10-02 23:29:42 | INFO     | Training TADR-VAE (seed=42)\n",
      "2025-10-02 23:30:54 | INFO     | âœ… Completed TADR-VAE (seed=42) in 71.44s\n",
      "2025-10-02 23:30:54 | INFO     | Training Vanilla VAE (seed=42)\n",
      "2025-10-02 23:31:27 | INFO     | âœ… Completed Vanilla VAE (seed=42) in 33.93s\n",
      "2025-10-02 23:31:27 | INFO     | Training VAE+EWC (seed=42)\n",
      "2025-10-02 23:32:17 | INFO     | âœ… Completed VAE+EWC (seed=42) in 49.05s\n",
      "2025-10-02 23:32:17 | INFO     | Running experiments with seed: 1337\n",
      "2025-10-02 23:32:17 | INFO     | Training TADR-VAE (seed=1337)\n",
      "2025-10-02 23:33:21 | INFO     | âœ… Completed TADR-VAE (seed=1337) in 64.02s\n",
      "2025-10-02 23:33:21 | INFO     | Training Vanilla VAE (seed=1337)\n",
      "2025-10-02 23:34:01 | INFO     | âœ… Completed Vanilla VAE (seed=1337) in 40.33s\n",
      "2025-10-02 23:34:01 | INFO     | Training VAE+EWC (seed=1337)\n",
      "2025-10-02 23:34:47 | INFO     | âœ… Completed VAE+EWC (seed=1337) in 46.44s\n",
      "2025-10-02 23:34:47 | INFO     | Running experiments with seed: 2024\n",
      "2025-10-02 23:34:47 | INFO     | Training TADR-VAE (seed=2024)\n",
      "2025-10-02 23:36:01 | INFO     | âœ… Completed TADR-VAE (seed=2024) in 73.13s\n",
      "2025-10-02 23:36:01 | INFO     | Training Vanilla VAE (seed=2024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use vector backend for high-quality PDFs\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"figure.titlesize\": 14,\n",
    "    \"text.usetex\": False, \n",
    "    \"font.family\": \"serif\",\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Configuration & Global Constants\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CONFIG = {\n",
    "    # Reproducibility\n",
    "    \"SEEDS\": [42, 1337, 2024],\n",
    "    \n",
    "    # Hardware\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \n",
    "    # Data\n",
    "    \"DATA_DIR\": Path(\"research_data\"),  \n",
    "    \"TASK_DEFINITIONS\": [\n",
    "        [\"dos\"],          \n",
    "        [\"probe\"],       \n",
    "        [\"r2l\", \"u2r\"],   \n",
    "    ],\n",
    "    \"MAX_SAMPLES_PER_TASK\": 10_000,\n",
    "    \n",
    "    # Training\n",
    "    \"EPOCHS\": 5,\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    \"TUNING_TRIALS\": 10,\n",
    "    \"TUNING_EPOCHS\": 3,\n",
    "}\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Ensure full reproducibility across runs.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Model Definitions\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class TADR_VAE(nn.Module):\n",
    "    \"\"\"Task-Aware Dynamic Reconstruction Variational Autoencoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, num_tasks: int, hparams: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_tasks = num_tasks\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.task_embeddings = nn.Embedding(num_tasks, hparams[\"task_embedding_dim\"])\n",
    "        self.task_emb_to_hidden = nn.Linear(hparams[\"task_embedding_dim\"], hparams[\"hidden_dim\"])\n",
    "        self.temporal_gating = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hparams[\"hidden_dim\"],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.gate_generator = nn.Sequential(\n",
    "            nn.Linear(input_dim + hparams[\"hidden_dim\"], input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hparams[\"hidden_dim\"]),\n",
    "            nn.LayerNorm(hparams[\"hidden_dim\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hparams[\"hidden_dim\"], hparams[\"hidden_dim\"] // 2),\n",
    "        )\n",
    "        self.latent_mu = nn.Linear(hparams[\"hidden_dim\"] // 2, hparams[\"latent_dim\"])\n",
    "        self.latent_logvar = nn.Linear(hparams[\"hidden_dim\"] // 2, hparams[\"latent_dim\"])\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hparams[\"latent_dim\"], hparams[\"hidden_dim\"] // 2),\n",
    "            nn.LayerNorm(hparams[\"hidden_dim\"] // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hparams[\"hidden_dim\"] // 2, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task_id: torch.Tensor):\n",
    "        # task_id: [B] â€” same batch size as x\n",
    "        task_emb = self.task_embeddings(task_id)  # [B, emb_dim]\n",
    "        h0 = self.task_emb_to_hidden(task_emb).unsqueeze(0)  # [1, B, hidden_dim]\n",
    "        gru_out, _ = self.temporal_gating(x.unsqueeze(1), h0)  # x: [B, 1, input_dim]\n",
    "        gating_weights = self.gate_generator(torch.cat([x, gru_out.squeeze(1)], dim=-1))\n",
    "        gated_input = x * gating_weights\n",
    "        encoded = self.encoder(gated_input)\n",
    "        mu, logvar = self.latent_mu(encoded), self.latent_logvar(encoded)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar, gating_weights\n",
    "\n",
    "    def _reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        recon: torch.Tensor,\n",
    "        mu: torch.Tensor,\n",
    "        logvar: torch.Tensor,\n",
    "        gate: torch.Tensor,\n",
    "        kl_weight: float,\n",
    "    ) -> torch.Tensor:\n",
    "        recon_loss = F.mse_loss(recon * gate, x * gate, reduction=\"mean\")\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kl_weight * kl_loss\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "    \"\"\"Baseline Variational Autoencoder (no task awareness).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hparams: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hparams[\"hidden_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hparams[\"hidden_dim\"], hparams[\"hidden_dim\"] // 2),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hparams[\"hidden_dim\"] // 2, hparams[\"latent_dim\"])\n",
    "        self.fc_logvar = nn.Linear(hparams[\"hidden_dim\"] // 2, hparams[\"latent_dim\"])\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hparams[\"latent_dim\"], hparams[\"hidden_dim\"] // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hparams[\"hidden_dim\"] // 2, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task_id: Optional[torch.Tensor] = None):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar, torch.ones_like(x)\n",
    "\n",
    "    def _reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        recon: torch.Tensor,\n",
    "        mu: torch.Tensor,\n",
    "        logvar: torch.Tensor,\n",
    "        gate: torch.Tensor,\n",
    "        kl_weight: float,\n",
    "    ) -> torch.Tensor:\n",
    "        recon_loss = F.mse_loss(recon, x, reduction=\"mean\")\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kl_weight * kl_loss\n",
    "\n",
    "\n",
    "class EWC(nn.Module):\n",
    "    \"\"\"Elastic Weight Consolidation wrapper for continual learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, ewc_lambda: float):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.tasks: Dict[int, Dict[str, Dict[str, torch.Tensor]]] = {}\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task_id: Optional[torch.Tensor] = None):\n",
    "        return self.model(x, task_id)\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        recon: torch.Tensor,\n",
    "        mu: torch.Tensor,\n",
    "        logvar: torch.Tensor,\n",
    "        gate: torch.Tensor,\n",
    "        kl_weight: float,\n",
    "    ) -> torch.Tensor:\n",
    "        base_loss = self.model.compute_loss(x, recon, mu, logvar, gate, kl_weight)\n",
    "        return base_loss + self.ewc_lambda * self.penalty()\n",
    "\n",
    "    def penalty(self) -> torch.Tensor:\n",
    "        if not self.tasks:\n",
    "            return torch.tensor(0.0, device=next(self.parameters()).device)\n",
    "        penalty = torch.tensor(0.0, device=next(self.parameters()).device)\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            for task_data in self.tasks.values():\n",
    "                _mu = task_data[\"mean\"][n]\n",
    "                _fisher = task_data[\"fisher\"][n]\n",
    "                penalty += (_fisher * (p - _mu).pow(2)).sum()\n",
    "        return penalty\n",
    "\n",
    "    def end_task(self, dataloader: DataLoader, task_id: int, kl_weight: float):\n",
    "        device = next(self.parameters()).device\n",
    "        fisher = {}\n",
    "        mean = {}\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                fisher[n] = torch.zeros_like(p.data, device=device)\n",
    "                mean[n] = p.data.clone().detach()\n",
    "\n",
    "        self.model.eval()\n",
    "        total_samples = 0\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            self.model.zero_grad()\n",
    "            # Pass task_id with correct batch size\n",
    "            task_tensor = torch.full((batch_size,), task_id, dtype=torch.long, device=device)\n",
    "            recon, mu, logvar, gate = self.model(x, task_tensor)\n",
    "            loss = self.model.compute_loss(x, recon, mu, logvar, gate, kl_weight)\n",
    "            loss.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.data.pow(2) * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "        for n in fisher:\n",
    "            fisher[n] /= total_samples\n",
    "        self.tasks[task_id] = {\"mean\": mean, \"fisher\": fisher}\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Data Loader: Handles Numeric Attack Labels\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class RealDatasetLoader:\n",
    "    \"\"\"Loads NSL-KDD from .txt files with numeric attack labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path = CONFIG[\"DATA_DIR\"]):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        train_path = self.data_dir / \"KDDTrain+.txt\"\n",
    "        test_path = self.data_dir / \"KDDTest+.txt\"\n",
    "        \n",
    "        if not train_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing: {train_path}\")\n",
    "        if not test_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing: {test_path}\")\n",
    "        logger.info(f\"âœ… Raw NSL-KDD files found in: {self.data_dir}\")\n",
    "\n",
    "    def get_dataset_tasks(self) -> List[Dict[str, np.ndarray]]:\n",
    "        \"\"\"Load and partition NSL-KDD into continual learning tasks.\"\"\"\n",
    "        logger.info(\"Loading NSL-KDD from .txt files...\")\n",
    "        \n",
    "        # Column schema (41 features + 2 labels)\n",
    "        columns = [\n",
    "            'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "            'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "            'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "            'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "            'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "            'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "            'same_srv_rate_2', 'diff_srv_rate_2', 'dst_host_count', 'dst_host_srv_count',\n",
    "            'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "            'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "            'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'\n",
    "        ]\n",
    "        \n",
    "        # Load raw data\n",
    "        train_df = pd.read_csv(self.data_dir / \"KDDTrain+.txt\", names=columns)\n",
    "        test_df = pd.read_csv(self.data_dir / \"KDDTest+.txt\", names=columns)\n",
    "        df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "        logger.info(f\"Combined dataset: {len(df)} samples\")\n",
    "\n",
    "        # Numeric attack code to category mapping (NSL-KDD standard)\n",
    "        numeric_attack_mapping = {\n",
    "            0: 'normal',\n",
    "            1: 'dos',      # back\n",
    "            2: 'u2r',      # buffer_overflow\n",
    "            3: 'r2l',      # ftp_write\n",
    "            4: 'r2l',      # guess_passwd\n",
    "            5: 'r2l',      # imap\n",
    "            6: 'probe',    # ipsweep\n",
    "            7: 'dos',      # land\n",
    "            8: 'u2r',      # loadmodule\n",
    "            9: 'r2l',      # multihop\n",
    "            10: 'dos',     # neptune\n",
    "            11: 'probe',   # nmap\n",
    "            12: 'u2r',     # perl\n",
    "            13: 'r2l',     # phf\n",
    "            14: 'dos',     # pod\n",
    "            15: 'probe',   # portsweep\n",
    "            16: 'u2r',     # rootkit\n",
    "            17: 'probe',   # satan\n",
    "            18: 'dos',     # smurf\n",
    "            19: 'r2l',     # spy\n",
    "            20: 'dos',     # teardrop\n",
    "            21: 'r2l',     # warezclient\n",
    "            22: 'r2l',     # warezmaster\n",
    "            23: 'dos',     # apache2\n",
    "            24: 'dos',     # udpstorm\n",
    "            25: 'dos',     # procestable\n",
    "            26: 'dos',     # mailbomb\n",
    "            27: 'probe',   # saint\n",
    "            28: 'probe',   # mscan\n",
    "            29: 'u2r',     # xterm\n",
    "            30: 'u2r',     # ps\n",
    "            31: 'u2r',     # sqlattack\n",
    "            32: 'r2l',     # httptunnel\n",
    "            33: 'r2l',     # sendmail\n",
    "            34: 'r2l',     # named\n",
    "            35: 'r2l',     # snmpgetattack\n",
    "            36: 'r2l',     # snmpguess\n",
    "            37: 'r2l',     # xlock\n",
    "            38: 'r2l',     # xsnoop\n",
    "        }\n",
    "\n",
    "        # Ensure 'attack' column is numeric\n",
    "        df['attack'] = pd.to_numeric(df['attack'], errors='coerce')\n",
    "        \n",
    "        # Map to categories\n",
    "        df['attack_cat'] = df['attack'].map(numeric_attack_mapping)\n",
    "        df = df.dropna(subset=['attack_cat'])  # Remove unmapped entries\n",
    "        logger.info(f\"After filtering: {len(df)} samples\")\n",
    "\n",
    "        # Prepare features\n",
    "        X_raw = df.drop(columns=['attack', 'level', 'attack_cat'])\n",
    "        y_str = df['attack_cat']\n",
    "\n",
    "        # One-hot encode categorical features\n",
    "        X_processed = pd.get_dummies(\n",
    "            X_raw,\n",
    "            columns=X_raw.select_dtypes(include=['object']).columns,\n",
    "            drop_first=False\n",
    "        )\n",
    "\n",
    "        # Global scaling\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X_processed)\n",
    "\n",
    "        # Create tasks\n",
    "        tasks = []\n",
    "        for task_idx, attack_types in enumerate(CONFIG[\"TASK_DEFINITIONS\"]):\n",
    "            mask = y_str.isin(attack_types) | (y_str == 'normal')\n",
    "            X_task = X_scaled[mask]\n",
    "            y_task = (y_str[mask].isin(attack_types)).astype(int).values\n",
    "\n",
    "            if len(X_task) > CONFIG[\"MAX_SAMPLES_PER_TASK\"]:\n",
    "                idx = np.random.choice(len(X_task), CONFIG[\"MAX_SAMPLES_PER_TASK\"], replace=False)\n",
    "                X_task, y_task = X_task[idx], y_task[idx]\n",
    "\n",
    "            tasks.append({\"X\": X_task.astype(np.float32), \"y\": y_task.astype(np.int64)})\n",
    "            logger.info(f\"Task {task_idx}: {len(X_task)} samples ({y_task.sum()} anomalies)\")\n",
    "\n",
    "        return tasks\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Hyperparameter Tuning\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"Optuna-based hyperparameter optimization for reconstruction-based anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_names: List[str], tuning_data: Dict[str, np.ndarray]):\n",
    "        self.model_names = model_names\n",
    "        self.X_tune = torch.from_numpy(tuning_data[\"X\"])\n",
    "        self.y_tune = tuning_data[\"y\"]\n",
    "        self.input_dim = self.X_tune.shape[1]\n",
    "        self.device = CONFIG[\"DEVICE\"]\n",
    "\n",
    "    def _objective(self, trial: optuna.Trial, model_name: str) -> float:\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        latent_dim = trial.suggest_categorical(\"latent_dim\", [16, 32, 64])\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "        kl_weight = trial.suggest_float(\"kl_weight\", 0.05, 0.5, log=True)\n",
    "\n",
    "        hparams = {\n",
    "            \"latent_dim\": latent_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"kl_weight\": kl_weight,\n",
    "        }\n",
    "\n",
    "        if model_name == \"TADR-VAE\":\n",
    "            task_emb_dim = trial.suggest_categorical(\"task_embedding_dim\", [8, 16])\n",
    "            hparams[\"task_embedding_dim\"] = task_emb_dim\n",
    "            model = TADR_VAE(self.input_dim, num_tasks=3, hparams=hparams).to(self.device)\n",
    "        elif model_name == \"Vanilla VAE\":\n",
    "            model = VanillaVAE(self.input_dim, hparams=hparams).to(self.device)\n",
    "        elif model_name == \"VAE+EWC\":\n",
    "            ewc_lambda = trial.suggest_float(\"ewc_lambda\", 100, 1000, log=True)\n",
    "            base_model = VanillaVAE(self.input_dim, hparams=hparams).to(self.device)\n",
    "            model = EWC(base_model, ewc_lambda).to(self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        dataset = TensorDataset(self.X_tune, torch.from_numpy(self.y_tune))\n",
    "        loader = DataLoader(dataset, batch_size=CONFIG[\"BATCH_SIZE\"], shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for _ in range(CONFIG[\"TUNING_EPOCHS\"]):\n",
    "            for batch_x, _ in loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                # âœ… FIXED: task_id matches batch size\n",
    "                task_id = torch.full((batch_x.size(0),), 0, dtype=torch.long, device=self.device)\n",
    "                recon, mu, logvar, gate = model(batch_x, task_id)\n",
    "                loss = model.compute_loss(batch_x, recon, mu, logvar, gate, kl_weight)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in loader:\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                # âœ… FIXED: task_id matches batch size\n",
    "                task_id = torch.full((batch_x.size(0),), 0, dtype=torch.long, device=self.device)\n",
    "                recon, _, _, _ = model(batch_x, task_id)\n",
    "                err = torch.mean((batch_x - recon) ** 2, dim=1).cpu().numpy()\n",
    "                errors.extend(err)\n",
    "        errors = np.array(errors)\n",
    "        return roc_auc_score(self.y_tune, errors)\n",
    "\n",
    "    def tune(self) -> Dict[str, Dict[str, Any]]:\n",
    "        logger.info(\"Starting hyperparameter tuning...\")\n",
    "        best_params = {}\n",
    "        for name in self.model_names:\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(\n",
    "                lambda trial: self._objective(trial, name),\n",
    "                n_trials=CONFIG[\"TUNING_TRIALS\"],\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            best_params[name] = study.best_params\n",
    "            logger.info(f\"Best params for {name}: {study.best_params}\")\n",
    "        return best_params\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Experiment Framework with Visualization\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class AdvancedExperimentFramework:\n",
    "    def _get_reconstruction_errors(\n",
    "        self, model: nn.Module, X_test: np.ndarray, task_id: int\n",
    "    ) -> np.ndarray:\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        dataset = TensorDataset(torch.from_numpy(X_test), torch.zeros(len(X_test)))\n",
    "        loader = DataLoader(dataset, batch_size=CONFIG[\"BATCH_SIZE\"])\n",
    "        errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                # âœ… FIXED: task_id matches batch size\n",
    "                task_tensor = torch.full((batch_x.size(0),), task_id, dtype=torch.long, device=device)\n",
    "                recon, _, _, _ = model(batch_x, task_tensor)\n",
    "                err = torch.mean((batch_x - recon) ** 2, dim=1).cpu().numpy()\n",
    "                errors.extend(err)\n",
    "        return np.array(errors)\n",
    "\n",
    "    def _find_optimal_threshold(self, y_true: np.ndarray, errors: np.ndarray) -> float:\n",
    "        thresholds = np.linspace(errors.min(), errors.max(), 100)\n",
    "        f1_scores = [f1_score(y_true, errors >= t) for t in thresholds]\n",
    "        return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "    def run(self):\n",
    "        logger.info(\"ðŸš€ Starting Advanced Experiment Framework\")\n",
    "        data_loader = RealDatasetLoader()\n",
    "        tasks = data_loader.get_dataset_tasks()\n",
    "        input_dim = tasks[0][\"X\"].shape[1]\n",
    "        num_tasks = len(tasks)\n",
    "\n",
    "        tuner = HyperparameterTuner(\n",
    "            model_names=[\"TADR-VAE\", \"Vanilla VAE\", \"VAE+EWC\"],\n",
    "            tuning_data=tasks[0]\n",
    "        )\n",
    "        best_params = tuner.tune()\n",
    "\n",
    "        full_results = {name: [] for name in best_params}\n",
    "\n",
    "        for seed in CONFIG[\"SEEDS\"]:\n",
    "            set_seed(seed)\n",
    "            logger.info(f\"Running experiments with seed: {seed}\")\n",
    "            for model_name in best_params:\n",
    "                params = best_params[model_name]\n",
    "                logger.info(f\"Training {model_name} (seed={seed})\")\n",
    "\n",
    "                if model_name == \"TADR-VAE\":\n",
    "                    model = TADR_VAE(input_dim, num_tasks, params).to(CONFIG[\"DEVICE\"])\n",
    "                elif model_name == \"Vanilla VAE\":\n",
    "                    model = VanillaVAE(input_dim, params).to(CONFIG[\"DEVICE\"])\n",
    "                elif model_name == \"VAE+EWC\":\n",
    "                    base = VanillaVAE(input_dim, params).to(CONFIG[\"DEVICE\"])\n",
    "                    model = EWC(base, params[\"ewc_lambda\"]).to(CONFIG[\"DEVICE\"])\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "                perf_matrix = np.zeros((num_tasks, num_tasks, 4))\n",
    "                start_time = time.time()\n",
    "\n",
    "                for task_id in range(num_tasks):\n",
    "                    X_train, y_train = tasks[task_id][\"X\"], tasks[task_id][\"y\"]\n",
    "                    train_loader = DataLoader(\n",
    "                        TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
    "                        batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "                        shuffle=True\n",
    "                    )\n",
    "                    model.train()\n",
    "                    for epoch in range(CONFIG[\"EPOCHS\"]):\n",
    "                        for batch_x, _ in train_loader:\n",
    "                            batch_x = batch_x.to(CONFIG[\"DEVICE\"])\n",
    "                            optimizer.zero_grad()\n",
    "                            # âœ… FIXED: task_id matches batch size\n",
    "                            tid_tensor = torch.full((batch_x.size(0),), task_id, dtype=torch.long, device=CONFIG[\"DEVICE\"])\n",
    "                            recon, mu, logvar, gate = model(batch_x, tid_tensor)\n",
    "                            loss = model.compute_loss(\n",
    "                                batch_x, recon, mu, logvar, gate, params[\"kl_weight\"]\n",
    "                            )\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    if isinstance(model, EWC):\n",
    "                        model.end_task(train_loader, task_id, params[\"kl_weight\"])\n",
    "\n",
    "                    for eval_task in range(task_id + 1):\n",
    "                        errors = self._get_reconstruction_errors(\n",
    "                            model, tasks[eval_task][\"X\"], eval_task\n",
    "                        )\n",
    "                        thresh = self._find_optimal_threshold(tasks[eval_task][\"y\"], errors)\n",
    "                        y_pred = (errors >= thresh).astype(int)\n",
    "                        y_true = tasks[eval_task][\"y\"]\n",
    "                        perf_matrix[task_id, eval_task, 0] = precision_score(y_true, y_pred)\n",
    "                        perf_matrix[task_id, eval_task, 1] = recall_score(y_true, y_pred)\n",
    "                        perf_matrix[task_id, eval_task, 2] = f1_score(y_true, y_pred)\n",
    "                        perf_matrix[task_id, eval_task, 3] = roc_auc_score(y_true, errors)\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                full_results[model_name].append({\n",
    "                    \"seed\": seed,\n",
    "                    \"performance_matrix\": perf_matrix,\n",
    "                    \"training_time_sec\": elapsed\n",
    "                })\n",
    "                logger.info(f\"âœ… Completed {model_name} (seed={seed}) in {elapsed:.2f}s\")\n",
    "\n",
    "        self.results = full_results\n",
    "        logger.info(\" All experiments completed successfully.\")\n",
    "        \n",
    "        # âœ… Generate Springer-ready output\n",
    "        self.save_results_and_figures()\n",
    "\n",
    "    def save_results_and_figures(self, output_dir: str = \"results\"):\n",
    "        \"\"\"Generate publication-ready figures and tables (Springer format).\"\"\"\n",
    "        from pathlib import Path\n",
    "\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Aggregate results: mean and std across seeds\n",
    "        summary = {}\n",
    "        for model_name, runs in self.results.items():\n",
    "            perf = np.array([run[\"performance_matrix\"] for run in runs])  # [seeds, T, T, 4]\n",
    "            mean_perf = np.mean(perf, axis=0)\n",
    "            std_perf = np.std(perf, axis=0)\n",
    "            summary[model_name] = {\"mean\": mean_perf, \"std\": std_perf}\n",
    "\n",
    "        # 1. Final Average Metrics Table (LaTeX-ready)\n",
    "        with open(output_path / \"results_table.tex\", \"w\") as f:\n",
    "            f.write(\"\\\\begin{table}[ht]\\n\\\\centering\\n\\\\caption{Final Performance (Mean $\\\\pm$ Std)}\\n\")\n",
    "            f.write(\"\\\\begin{tabular}{lcccc}\\n\\\\toprule\\nModel & Precision & Recall & F1-Score & AUC \\\\\\\\\\n\\\\midrule\\n\")\n",
    "            for model_name in summary:\n",
    "                mean = summary[model_name][\"mean\"]\n",
    "                prec = np.mean(mean[-1, :, 0])\n",
    "                rec = np.mean(mean[-1, :, 1])\n",
    "                f1 = np.mean(mean[-1, :, 2])\n",
    "                auc = np.mean(mean[-1, :, 3])\n",
    "                f.write(f\"{model_name} & {prec:.3f} & {rec:.3f} & {f1:.3f} & {auc:.3f} \\\\\\\\\\n\")\n",
    "            f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\")\n",
    "\n",
    "        # 2. F1-Score Over Tasks (Continual Learning Plot)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        for model_name in summary:\n",
    "            mean_f1 = summary[model_name][\"mean\"][:, :, 2]\n",
    "            diag_f1 = np.diag(mean_f1)\n",
    "            plt.plot(range(len(diag_f1)), diag_f1, marker='o', label=model_name, linewidth=2)\n",
    "        plt.xlabel(\"Task Index\")\n",
    "        plt.ylabel(\"F1-Score\")\n",
    "        plt.title(\"Continual Learning Performance\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path / \"f1_over_tasks.pdf\", dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(output_path / \"f1_over_tasks.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Forgetting Metric\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        for model_name in summary:\n",
    "            mean_f1 = summary[model_name][\"mean\"][:, :, 2]\n",
    "            num_tasks = mean_f1.shape[0]\n",
    "            forgetting = []\n",
    "            for t in range(1, num_tasks):\n",
    "                max_perf = np.max(mean_f1[:t+1, t])\n",
    "                final_perf = mean_f1[t, t]\n",
    "                forget = max_perf - final_perf\n",
    "                forgetting.append(forget)\n",
    "            if forgetting:\n",
    "                plt.plot(range(1, num_tasks), forgetting, marker='s', label=model_name, linewidth=2)\n",
    "        plt.xlabel(\"Task Index\")\n",
    "        plt.ylabel(\"Forgetting (Î” F1)\")\n",
    "        plt.title(\"Catastrophic Forgetting Analysis\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path / \"forgetting.pdf\", dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(output_path / \"forgetting.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # 4. Final Performance Heatmap\n",
    "        for model_name in summary:\n",
    "            plt.figure(figsize=(6, 2))\n",
    "            final_f1 = summary[model_name][\"mean\"][-1, :, 2]\n",
    "            sns.heatmap(\n",
    "                final_f1.reshape(1, -1),\n",
    "                annot=True,\n",
    "                fmt=\".3f\",\n",
    "                cmap=\"viridis\",\n",
    "                xticklabels=[f\"Task {i}\" for i in range(len(final_f1))],\n",
    "                yticklabels=[\"Final\"],\n",
    "                cbar_kws={'label': 'F1-Score'}\n",
    "            )\n",
    "            plt.title(f\"Final Task Performance: {model_name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path / f\"heatmap_{model_name.replace(' ', '_')}.pdf\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        logger.info(f\" figures and tables saved to: {output_path.resolve()}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Entry Point\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        framework = AdvancedExperimentFramework()\n",
    "        framework.run()\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Fatal error during execution:\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541079d2-fbb8-4be2-8210-d77294ab3b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
