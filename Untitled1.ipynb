{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab8b0c-0fee-4bbd-a882-b674c4bde307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TADR-VAE v17.0: Advanced Research Engine for Continual Learning Anomaly Detection\n",
    "Corrected Version: GRU hidden size and Optuna trial fixed.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import optuna\n",
    "\n",
    "# --- Config ---\n",
    "CONFIG = {\n",
    "    \"SEEDS\": [42, 1337, 2024],\n",
    "    \"DEVICE\": 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"DATASETS\": ['NSL-KDD'],\n",
    "    \"TUNING_TRIALS\": 10,\n",
    "    \"EPOCHS\": 5,\n",
    "    \"BATCH_SIZE\": 256,\n",
    "}\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    torch.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Models\n",
    "# =============================================================================\n",
    "class TADR_VAE(nn.Module):\n",
    "    def __init__(self, input_dim, num_tasks, p):\n",
    "        super().__init__()\n",
    "        self.task_embeddings = nn.Embedding(num_tasks, p['task_embedding_dim'])\n",
    "        self.task_emb_to_hidden = nn.Linear(p['task_embedding_dim'], p['hidden_dim'])\n",
    "        self.temporal_gating = nn.GRU(input_size=input_dim, hidden_size=p['hidden_dim'], batch_first=True)\n",
    "        self.gate_generator = nn.Sequential(nn.Linear(input_dim + p['hidden_dim'], input_dim), nn.Sigmoid())\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, p['hidden_dim']), nn.LayerNorm(p['hidden_dim']), nn.GELU(), nn.Linear(p['hidden_dim'], p['hidden_dim']//2))\n",
    "        self.latent_mu = nn.Linear(p['hidden_dim']//2, p['latent_dim'])\n",
    "        self.latent_logvar = nn.Linear(p['hidden_dim']//2, p['latent_dim'])\n",
    "        self.decoder = nn.Sequential(nn.Linear(p['latent_dim'], p['hidden_dim']//2), nn.LayerNorm(p['hidden_dim']//2), nn.GELU(), nn.Linear(p['hidden_dim']//2, input_dim))\n",
    "\n",
    "    def forward(self, x, task_id):\n",
    "        task_emb = self.task_embeddings(task_id)\n",
    "        h0 = self.task_emb_to_hidden(task_emb).unsqueeze(0).repeat(1, x.size(0), 1)  # Fix hidden size\n",
    "        gru_out, _ = self.temporal_gating(x.unsqueeze(1), h0)\n",
    "        gating_weights = self.gate_generator(torch.cat([x, gru_out.squeeze(1)], dim=-1))\n",
    "        gated_input = x * gating_weights\n",
    "        encoded = self.encoder(gated_input)\n",
    "        mu, logvar = self.latent_mu(encoded), self.latent_logvar(encoded)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar, gating_weights\n",
    "\n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def compute_loss(self, x, recon, mu, logvar, gate, kl_weight):\n",
    "        recon_loss = F.mse_loss(recon * gate, x * gate)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kl_weight * kl_loss\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "    def __init__(self, input_dim, p):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, p['hidden_dim']), nn.ReLU(), nn.Linear(p['hidden_dim'], p['hidden_dim']//2))\n",
    "        self.fc_mu = nn.Linear(p['hidden_dim']//2, p['latent_dim'])\n",
    "        self.fc_logvar = nn.Linear(p['hidden_dim']//2, p['latent_dim'])\n",
    "        self.decoder = nn.Sequential(nn.Linear(p['latent_dim'], p['hidden_dim']//2), nn.ReLU(), nn.Linear(p['hidden_dim']//2, input_dim))\n",
    "\n",
    "    def forward(self, x, task_id=None):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar, torch.ones_like(x)\n",
    "\n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def compute_loss(self, x, recon, mu, logvar, gate, kl_weight):\n",
    "        recon_loss = F.mse_loss(recon, x)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kl_weight * kl_loss\n",
    "\n",
    "class EWC(nn.Module):\n",
    "    def __init__(self, model, ewc_lambda):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.tasks = {}\n",
    "\n",
    "    def forward(self, x, task_id=None):\n",
    "        return self.model(x, task_id)\n",
    "\n",
    "    def compute_loss(self, x, recon, mu, logvar, gate, kl_weight):\n",
    "        return self.model.compute_loss(x, recon, mu, logvar, gate, kl_weight) + self.ewc_lambda * self.penalty()\n",
    "\n",
    "    def penalty(self):\n",
    "        penalty = 0.\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                for task_id, task_data in self.tasks.items():\n",
    "                    penalty += (task_data['fisher'][n] * (p - task_data['mean'][n]).pow(2)).sum()\n",
    "        return penalty\n",
    "\n",
    "    def end_task(self, dataloader, task_id, kl_weight):\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        mean = {n: p.clone().detach() for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self.model.eval()\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(CONFIG['DEVICE'])\n",
    "            self.model.zero_grad()\n",
    "            recon, mu, logvar, gate = self.model(x, task_id)\n",
    "            loss = self.model.compute_loss(x, recon, mu, logvar, gate, kl_weight)\n",
    "            loss.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    fisher[n] += p.grad.detach().pow(2) / len(dataloader.dataset)\n",
    "        self.tasks[task_id] = {'mean': mean, 'fisher': fisher}\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Data Loader\n",
    "# =============================================================================\n",
    "class RealDatasetLoader:\n",
    "    def __init__(self, data_dir=\"research_data\"):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "    def get_dataset_tasks(self, name='NSL-KDD'):\n",
    "        path = os.path.join(self.data_dir, \"nsl_kdd_processed.csv\")\n",
    "        if not os.path.exists(path):\n",
    "            self._download_and_process_nsl_kdd()\n",
    "        df = pd.read_csv(path)\n",
    "        X_df = df.drop(columns=['attack', 'level', 'attack_cat'])\n",
    "        y_str = df['attack_cat']\n",
    "        X_processed = pd.get_dummies(X_df, columns=X_df.select_dtypes(include=['object']).columns)\n",
    "        task_attacks = [['dos'], ['probe'], ['r2l','u2r']]\n",
    "        tasks = []\n",
    "        global_scaler = MinMaxScaler().fit(X_processed)\n",
    "        for attacks in task_attacks:\n",
    "            indices = y_str.isin(attacks) | (y_str == 'normal')\n",
    "            X_task, y_task_str = X_processed[indices], y_str[indices]\n",
    "            y_task = y_task_str.isin(attacks).astype(int)\n",
    "            X_sub, y_sub = self._subsample_data(X_task.values, y_task.values, 10000)\n",
    "            tasks.append({'X': global_scaler.transform(X_sub), 'y': y_sub})\n",
    "        return tasks\n",
    "\n",
    "    def _download_and_process_nsl_kdd(self):\n",
    "        logger.info(\"Downloading NSL-KDD...\")\n",
    "        # download and process here (same as previous code)\n",
    "        pass\n",
    "\n",
    "    def _subsample_data(self, X, y, n):\n",
    "        if len(X) <= n: return X, y\n",
    "        indices = np.random.choice(len(X), n, replace=False)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Hyperparameter Tuning\n",
    "# =============================================================================\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, models_to_tune, tuning_data):\n",
    "        self.models = models_to_tune\n",
    "        self.X_tune, self.y_tune = tuning_data['X'], tuning_data['y']\n",
    "        self.input_dim = self.X_tune.shape[1]\n",
    "\n",
    "    def _objective(self, trial, model_name):\n",
    "        if model_name==\"TADR-VAE\":\n",
    "            params = {\n",
    "                'lr': trial.suggest_loguniform('lr',1e-4,1e-2),\n",
    "                'latent_dim': trial.suggest_categorical('latent_dim',[16,32,64]),\n",
    "                'hidden_dim': trial.suggest_categorical('hidden_dim',[64,128,256]),\n",
    "                'task_embedding_dim': trial.suggest_categorical('task_embedding_dim',[8,16]),\n",
    "                'kl_weight': trial.suggest_loguniform('kl_weight',0.05,0.5)\n",
    "            }\n",
    "            model = TADR_VAE(self.input_dim,3,params).to(CONFIG['DEVICE'])\n",
    "        elif model_name==\"Vanilla VAE\":\n",
    "            params = {\n",
    "                'lr': trial.suggest_loguniform('lr',1e-4,1e-2),\n",
    "                'latent_dim': trial.suggest_categorical('latent_dim',[16,32,64]),\n",
    "                'hidden_dim': trial.suggest_categorical('hidden_dim',[64,128,256]),\n",
    "                'kl_weight': trial.suggest_loguniform('kl_weight',0.05,0.5)\n",
    "            }\n",
    "            model = VanillaVAE(self.input_dim, params).to(CONFIG['DEVICE'])\n",
    "        elif model_name==\"VAE+EWC\":\n",
    "            params = {\n",
    "                'lr': trial.suggest_loguniform('lr',1e-4,1e-2),\n",
    "                'latent_dim': trial.suggest_categorical('latent_dim',[16,32,64]),\n",
    "                'hidden_dim': trial.suggest_categorical('hidden_dim',[64,128,256]),\n",
    "                'kl_weight': trial.suggest_loguniform('kl_weight',0.05,0.5),\n",
    "                'ewc_lambda': trial.suggest_loguniform('ewc_lambda',100,1000)\n",
    "            }\n",
    "            base_model = VanillaVAE(self.input_dim, params).to(CONFIG['DEVICE'])\n",
    "            model = EWC(base_model, params['ewc_lambda'])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        train_loader = DataLoader(TensorDataset(torch.FloatTensor(self.X_tune), torch.LongTensor(self.y_tune)), batch_size=CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "        # Train for a few epochs\n",
    "        model.train()\n",
    "        for epoch in range(3):\n",
    "            for data, _ in train_loader:\n",
    "                data = data.to(CONFIG['DEVICE'])\n",
    "                optimizer.zero_grad()\n",
    "                recon, mu, logvar, gate = model(data, torch.LongTensor([0]).to(CONFIG['DEVICE']))\n",
    "                loss = model.compute_loss(data, recon, mu, logvar, gate, params.get('kl_weight',0.1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate reconstruction error\n",
    "        model.eval()\n",
    "        errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch, _ in train_loader:\n",
    "                batch = batch.to(CONFIG['DEVICE'])\n",
    "                recon, _, _, _ = model(batch, torch.LongTensor([0]).to(CONFIG['DEVICE']))\n",
    "                errors.extend(torch.mean((batch - recon)**2, dim=1).cpu().numpy())\n",
    "\n",
    "        return roc_auc_score(self.y_tune, np.array(errors))\n",
    "\n",
    "    def tune(self):\n",
    "        logger.info(\"\\n--- Hyperparameter Tuning Started ---\")\n",
    "        best_params = {}\n",
    "        for name in self.models:\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(lambda trial: self._objective(trial, name), n_trials=CONFIG['TUNING_TRIALS'])\n",
    "            best_params[name] = study.best_params\n",
    "            logger.info(f\"Best params for {name}: {study.best_params}\")\n",
    "        return best_params\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Advanced Experiment Framework\n",
    "# =============================================================================\n",
    "class AdvancedExperimentFramework:\n",
    "    def _get_reconstruction_errors(self, model, X_test, task_id):\n",
    "        model.eval(); errors = []\n",
    "        with torch.no_grad():\n",
    "            test_loader = DataLoader(TensorDataset(torch.FloatTensor(X_test), torch.zeros(len(X_test))), batch_size=CONFIG['BATCH_SIZE'])\n",
    "            for batch, _ in test_loader:\n",
    "                batch = batch.to(CONFIG['DEVICE'])\n",
    "                recon, _, _, _ = model(batch, task_id)\n",
    "                errors.extend(torch.mean((batch - recon)**2, dim=1).cpu().numpy())\n",
    "        return np.array(errors)\n",
    "\n",
    "    def _find_optimal_threshold(self, y_true, errors):\n",
    "        thresholds = np.linspace(np.min(errors), np.max(errors), 100)\n",
    "        f1s = [f1_score(y_true, errors >= t) for t in thresholds]\n",
    "        return thresholds[np.argmax(f1s)]\n",
    "\n",
    "    def run(self):\n",
    "        data_loader = RealDatasetLoader()\n",
    "        tasks = data_loader.get_dataset_tasks()\n",
    "        input_dim = tasks[0]['X'].shape[1]\n",
    "        num_tasks = len(tasks)\n",
    "\n",
    "        models_to_tune = [\"TADR-VAE\",\"Vanilla VAE\",\"VAE+EWC\"]\n",
    "        tuner = HyperparameterTuner(models_to_tune, tasks[0])\n",
    "        best_params = tuner.tune()\n",
    "\n",
    "        full_results = {name: [] for name in models_to_tune}\n",
    "\n",
    "        logger.info(\"\\n--- Main Experiments Started ---\")\n",
    "        for seed in CONFIG['SEEDS']:\n",
    "            set_seed(seed)\n",
    "            for name in models_to_tune:\n",
    "                params = best_params[name]\n",
    "                if name==\"TADR-VAE\":\n",
    "                    model = TADR_VAE(input_dim, num_tasks, params).to(CONFIG['DEVICE'])\n",
    "                elif name==\"Vanilla VAE\":\n",
    "                    model = VanillaVAE(input_dim, params).to(CONFIG['DEVICE'])\n",
    "                elif name==\"VAE+EWC\":\n",
    "                    base_model = VanillaVAE(input_dim, params).to(CONFIG['DEVICE'])\n",
    "                    model = EWC(base_model, params['ewc_lambda'])\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "                performance_matrix = np.zeros((num_tasks,num_tasks,4))\n",
    "                start_time = time.time()\n",
    "                for i in range(num_tasks):\n",
    "                    X_train, y_train = tasks[i]['X'], tasks[i]['y']\n",
    "                    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train)), batch_size=CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "                    task_id_tensor = torch.LongTensor([i]).to(CONFIG['DEVICE'])\n",
    "\n",
    "                    # Training\n",
    "                    model.train()\n",
    "                    for epoch in range(CONFIG['EPOCHS']):\n",
    "                        for batch_x, _ in train_loader:\n",
    "                            batch_x = batch_x.to(CONFIG['DEVICE'])\n",
    "                            optimizer.zero_grad()\n",
    "                            recon, mu, logvar, gate = model(batch_x, task_id_tensor)\n",
    "                            loss = model.compute_loss(batch_x, recon, mu, logvar, gate, params.get('kl_weight',0.1))\n",
    "                            loss.backward(); optimizer.step()\n",
    "\n",
    "                    if name==\"VAE+EWC\":\n",
    "                        model.end_task(train_loader, i, params.get('kl_weight',0.1))\n",
    "\n",
    "                    # Evaluate on all seen tasks\n",
    "                    for j in range(i+1):\n",
    "                        errors = self._get_reconstruction_errors(model, tasks[j]['X'], torch.LongTensor([j]).to(CONFIG['DEVICE']))\n",
    "                        threshold = self._find_optimal_threshold(tasks[j]['y'], errors)\n",
    "                        y_pred = (errors >= threshold).astype(int)\n",
    "                        performance_matrix[i,j,0] = precision_score(tasks[j]['y'], y_pred)\n",
    "                        performance_matrix[i,j,1] = recall_score(tasks[j]['y'], y_pred)\n",
    "                        performance_matrix[i,j,2] = f1_score(tasks[j]['y'], y_pred)\n",
    "                        performance_matrix[i,j,3] = roc_auc_score(tasks[j]['y'], errors)\n",
    "\n",
    "                full_results[name].append({'seed': seed, 'perf': performance_matrix, 'time': time.time()-start_time})\n",
    "                logger.info(f\"Completed {name} with seed {seed}\")\n",
    "\n",
    "        logger.info(\"\\n--- Experiments Completed ---\")\n",
    "        self.results = full_results\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Run\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    framework = AdvancedExperimentFramework()\n",
    "    framework.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7967b27-97eb-47e4-85ac-80f35ef2290e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
